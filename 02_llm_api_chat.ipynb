{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T18:18:19.415392Z",
     "start_time": "2025-06-11T18:18:19.148528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('utils', exist_ok=True)\n",
    "os.makedirs('results/part_2', exist_ok=True)"
   ],
   "id": "f1e2f1e775042708",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T22:06:10.200405Z",
     "start_time": "2025-06-10T22:06:10.061881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
    "headers = {\"Authorization\": \"Bearer\"}  # Replace with your actual API key\n",
    "def query(payload):\n",
    "    \"\"\"\n",
    "    Send a query to the Hugging Face API\n",
    "\n",
    "    Args:\n",
    "        payload: Dictionary containing the query parameters\n",
    "\n",
    "    Returns:\n",
    "        The API response as JSON\n",
    "    \"\"\"\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    # Print raw response content for debugging\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    print(\"Raw Response:\", response.text)\n",
    "\n",
    "    try:\n",
    "        return response.json()\n",
    "    except requests.exceptions.JSONDecodeError:\n",
    "        return {\"error\": \"Invalid JSON response\", \"raw_response\": response.text}\n",
    "\n",
    "# Test the query function\n",
    "test_payload = {\"inputs\": \"What are the symptoms of diabetes?\"}\n",
    "response = query(test_payload)\n",
    "print(response)"
   ],
   "id": "a743169136bcfcb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 404\n",
      "Raw Response: Not Found\n",
      "{'error': 'Invalid JSON response', 'raw_response': 'Not Found'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T23:40:48.291627Z",
     "start_time": "2025-06-01T23:40:48.253909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# utils/one_off_chat.py\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_response(prompt, model_name=\"google/flan-t5-base\", api_key=None):\n",
    "    \"\"\"\n",
    "    Get a response from the model\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt to send to the model\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication (optional for some models)\n",
    "\n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    headers = {}\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "\n",
    "    payload = {\"inputs\": prompt}\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if isinstance(data, list) and \"generated_text\" in data[0]:\n",
    "            return data[0][\"generated_text\"]\n",
    "        elif isinstance(data, list) and \"summary_text\" in data[0]:  # Some models use this\n",
    "            return data[0][\"summary_text\"]\n",
    "        elif isinstance(data, list):\n",
    "            return str(data[0])\n",
    "        else:\n",
    "            return str(data)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"[ERROR] API request failed: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] Unexpected error: {e}\"\n",
    "\n",
    "def run_chat(model_name, api_key):\n",
    "    \"\"\"Run an interactive chat session\"\"\"\n",
    "    print(f\"Welcome to the Simple LLM Chat with {model_name}! Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = get_response(user_input, model_name=model_name, api_key=api_key)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"google/flan-t5-base\", help=\"Hugging Face model name\")\n",
    "    parser.add_argument(\"--api-key\", type=str, default=os.getenv(\"HF_API_KEY\"), help=\"Hugging Face API key\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    run_chat(model_name=args.model, api_key=args.api_key)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "52fa81a1fb3a1927",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model MODEL] [--api-key API_KEY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\ericy\\AppData\\Roaming\\jupyter\\runtime\\kernel-1da419c8-d3b4-4497-856c-3d86707d96d6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[31mSystemExit\u001B[39m\u001B[31m:\u001B[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericy\\PycharmProjects\\DATASCI 223\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3675: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# utils/conversation.py\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_response(prompt, history=None, model_name=\"google/flan-t5-base\", api_key=None, history_length=3):\n",
    "    \"\"\"\n",
    "    Get a response from the model using conversation history\n",
    "    \n",
    "    Args:\n",
    "        prompt: The current user prompt\n",
    "        history: List of previous (prompt, response) tuples\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        history_length: Number of previous exchanges to include in context\n",
    "        \n",
    "    Returns:\n",
    "        The model's response\n",
    "    \"\"\"\n",
    "    # TODO: Implement the contextual response function\n",
    "    # Initialize history if None\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    # TODO: Format a prompt that includes previous exchanges\n",
    "    # Get a response from the API\n",
    "    # Return the response\n",
    "    pass\n",
    "\n",
    "def run_chat():\n",
    "    \"\"\"Run an interactive chat session with context\"\"\"\n",
    "    print(\"Welcome to the Contextual LLM Chat! Type 'exit' to quit.\")\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        # TODO: Get response using conversation history\n",
    "        # Update history\n",
    "        # Print the response\n",
    "        \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Chat with an LLM using conversation history\")\n",
    "    # TODO: Add arguments to the parser\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # TODO: Run the chat function with parsed arguments\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "ef1eb78ca388dfce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our chat modules - since we're in the same directory\n",
    "from one_off_chat import get_response as get_one_off_response\n",
    "# Optionally import the conversation module if testing that too\n",
    "# from conversation import get_response as get_contextual_response\n",
    "\n",
    "def test_chat(questions, model_name=\"google/flan-t5-base\", api_key=None):\n",
    "    \"\"\"\n",
    "    Test the chat function with a list of questions\n",
    "    \n",
    "    Args:\n",
    "        questions: A list of questions to test\n",
    "        model_name: Name of the model to use\n",
    "        api_key: API key for authentication\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping questions to responses\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"Testing question: {question}\")\n",
    "        # Get response using the one-off chat function\n",
    "        response = get_one_off_response(question, model_name, api_key)\n",
    "        results[question] = response\n",
    "        \n",
    "    return results\n",
    "\n",
    "# List of healthcare questions to test\n",
    "test_questions = [\n",
    "    \"What are the symptoms of gout?\",\n",
    "    \"How is gout diagnosed?\",\n",
    "    \"What treatments are available for gout?\",\n",
    "    \"What lifestyle changes can help manage gout?\",\n",
    "    \"What foods should be avoided with gout?\"\n",
    "]\n",
    "\n",
    "def save_results(results, output_file=\"results/part_2/example.txt\"):\n",
    "    \"\"\"\n",
    "    Save the test results to a file\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary mapping questions to responses\n",
    "        output_file: Path to the output file\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"# LLM Chat Tool Test Results\\n\\n\")\n",
    "        \n",
    "        # Write usage examples\n",
    "        f.write(\"## Usage Examples\\n\\n\")\n",
    "        f.write(\"```bash\\n\")\n",
    "        f.write(\"# Run the one-off chat\\n\")\n",
    "        f.write(\"python utils/one_off_chat.py\\n\\n\")\n",
    "        f.write(\"# Run the contextual chat\\n\")\n",
    "        f.write(\"python utils/conversation.py\\n\")\n",
    "        f.write(\"```\\n\\n\")\n",
    "        \n",
    "        # Write test results\n",
    "        f.write(\"## Test Results\\n\\n\")\n",
    "        f.write(\"```csv\\n\")\n",
    "        f.write(\"question,response\\n\")\n",
    "        \n",
    "        for question, response in results.items():\n",
    "            # Format the question and response for CSV\n",
    "            q = question.replace(',', '').replace('\\n', ' ')\n",
    "            r = response.replace(',', '').replace('\\n', ' ')\n",
    "            f.write(f\"{q},{r}\\n\")\n",
    "            \n",
    "        f.write(\"```\\n\")\n",
    "\n",
    "# Run the test and save results\n",
    "if __name__ == \"__main__\":\n",
    "    results = test_chat(test_questions)\n",
    "    save_results(results)\n",
    "    print(\"Test results saved to results/part_2/example.txt\")"
   ],
   "id": "711a166825840924"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
