{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T18:40:27.492696Z",
     "start_time": "2025-06-11T18:40:27.486650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)"
   ],
   "id": "cf0c8f5f0e4bb329",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T18:40:04.676649Z",
     "start_time": "2025-06-11T18:40:04.215340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's explore the Synthetic Mention Corpora for Disease Entity Recognition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data/synthetic_mentions', exist_ok=True)\n",
    "\n",
    "# Function to download the dataset\n",
    "def download_synthetic_mentions():\n",
    "    \"\"\"\n",
    "    Download the Synthetic Mention Corpora for Disease Entity Recognition\n",
    "    \n",
    "    Note: You need to manually download this dataset from PhysioNet:\n",
    "    https://physionet.org/content/synthetic-mention-corpora/\n",
    "    \n",
    "    After downloading, place the files in the data/synthetic_mentions directory\n",
    "    \"\"\"\n",
    "    mentions_data_path = 'data/synthetic_mentions/disease_mentions.json'\n",
    "    \n",
    "    if os.path.exists(mentions_data_path):\n",
    "        print(f\"Loading Synthetic Mention Corpora from {mentions_data_path}\")\n",
    "        with open(mentions_data_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Synthetic Mention Corpora not found at {mentions_data_path}\")\n",
    "        print(\"Please download the dataset from PhysioNet:\")\n",
    "        print(\"https://physionet.org/content/synthetic-mention-corpora/\")\n",
    "        print(\"After downloading, place the files in the data/synthetic_mentions directory\")\n",
    "        return None\n",
    "\n",
    "# Try to load the dataset\n",
    "mentions_data = download_synthetic_mentions()\n",
    "\n",
    "# If the dataset is loaded successfully, convert to text for training\n",
    "if mentions_data is not None:\n",
    "    # Extract mentions and combine into a single text\n",
    "    mentions_text = \"\"\n",
    "    for item in mentions_data[:1000]:  # Start with a subset for exploration\n",
    "        if \"mention\" in item:\n",
    "            mentions_text += item[\"mention\"] + \"\\n\"\n",
    "        if \"context\" in item:\n",
    "            mentions_text += item[\"context\"] + \"\\n\\n\"\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Total characters: {len(mentions_text)}\")\n",
    "    print(f\"Total words: {len(mentions_text.split())}\")\n",
    "    print(f\"Total lines: {len(mentions_text.splitlines())}\")\n",
    "    \n",
    "    # Print the first few lines\n",
    "    print(\"\\nFirst few lines:\")\n",
    "    for i, line in enumerate(mentions_text.splitlines()[:5]):\n",
    "        print(f\"{i+1}: {line}\")\n",
    "    \n",
    "    # Check if the data is suitable for training\n",
    "    if len(mentions_text) < 100000:  # Less than 100KB\n",
    "        print(\"\\nWarning: The extracted text might be too small for effective training.\")\n",
    "        print(\"Consider using more entries from the dataset.\")\n",
    "    else:\n",
    "        print(\"\\nThe extracted text seems suitable for training.\")\n",
    "    \n",
    "    # Save the combined text for training\n",
    "    with open('data/processed/mentions_text.txt', 'w') as f:\n",
    "        f.write(mentions_text)\n",
    "    print(\"Saved combined text to data/processed/mentions_text.txt\")\n",
    "else:\n",
    "    # Fallback to open_db.txt if the dataset is not available\n",
    "    print(\"Falling back to open_db.txt for training\")\n",
    "    \n",
    "    def read_open_db():\n",
    "        \"\"\"Read the open database text file\"\"\"\n",
    "        with open('open_db.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    \n",
    "    # Read the open database text\n",
    "    mentions_text = read_open_db()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Total characters: {len(mentions_text)}\")\n",
    "    print(f\"Total words: {len(mentions_text.split())}\")\n",
    "    print(f\"Total lines: {len(mentions_text.splitlines())}\")"
   ],
   "id": "baf33fad5ae4a928",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic Mention Corpora not found at data/synthetic_mentions/disease_mentions.json\n",
      "Please download the dataset from PhysioNet:\n",
      "https://physionet.org/content/synthetic-mention-corpora/\n",
      "After downloading, place the files in the data/synthetic_mentions directory\n",
      "Falling back to open_db.txt for training\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'open_db.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 79\u001B[39m\n\u001B[32m     76\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n\u001B[32m     78\u001B[39m \u001B[38;5;66;03m# Read the open database text\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m79\u001B[39m mentions_text = \u001B[43mread_open_db\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[38;5;66;03m# Print some statistics\u001B[39;00m\n\u001B[32m     82\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTotal characters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(mentions_text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 74\u001B[39m, in \u001B[36mread_open_db\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     72\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mread_open_db\u001B[39m():\n\u001B[32m     73\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Read the open database text file\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m74\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mopen_db.txt\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m     75\u001B[39m         text = f.read()\n\u001B[32m     76\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DATASCI 223\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    321\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    322\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    323\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    324\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'open_db.txt'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-11T18:41:42.253051Z",
     "start_time": "2025-06-11T18:41:42.199080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's preprocess the text data for training\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        \"\"\"Initialize the tokenizer with the training text\"\"\"\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size} characters\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode text to token IDs\"\"\"\n",
    "        return [self.stoi[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Decode token IDs to text\"\"\"\n",
    "        return ''.join([self.itos[id] for id in ids])\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the tokenizer to a file\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'chars': self.chars,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'stoi': self.stoi,\n",
    "                'itos': {str(k): v for k, v in self.itos.items()}  # Convert keys to strings for JSON\n",
    "            }, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load a tokenizer from a file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        tokenizer = cls.__new__(cls)\n",
    "        tokenizer.chars = data['chars']\n",
    "        tokenizer.vocab_size = data['vocab_size']\n",
    "        tokenizer.stoi = data['stoi']\n",
    "        tokenizer.itos = {int(k): v for k, v in data['itos'].items()}  # Convert keys back to integers\n",
    "        \n",
    "        return tokenizer\n",
    "\n",
    "# Create a tokenizer from the mentions text\n",
    "tokenizer = CharacterTokenizer(mentions_text)\n",
    "\n",
    "# Encode the entire text\n",
    "encoded_text = tokenizer.encode(mentions_text)\n",
    "print(f\"Encoded text length: {len(encoded_text)} tokens\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save('data/processed/char_tokenizer.json')\n",
    "print(\"Tokenizer saved to data/processed/char_tokenizer.json\")\n",
    "\n",
    "# Split the data into train and validation sets (90% train, 10% validation)\n",
    "train_size = int(0.9 * len(encoded_text))\n",
    "train_data = encoded_text[:train_size]\n",
    "val_data = encoded_text[train_size:]\n",
    "\n",
    "print(f\"Train data size: {len(train_data)} tokens\")\n",
    "print(f\"Validation data size: {len(val_data)} tokens\")\n",
    "\n",
    "# Save the processed data\n",
    "np.save('data/processed/train_data.npy', np.array(train_data, dtype=np.int16))\n",
    "np.save('data/processed/val_data.npy', np.array(val_data, dtype=np.int16))\n",
    "print(\"Processed data saved to data/processed/\")\n",
    "\n",
    "# Create a dataset class for training\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for training a language model\"\"\"\n",
    "    \n",
    "    def __init__(self, data, context_length=256):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            data: List of token IDs\n",
    "            context_length: Context length for prediction\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.context_length = context_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of possible contexts\"\"\"\n",
    "        return len(self.data) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a context and target pair\"\"\"\n",
    "        context = self.data[idx:idx+self.context_length]\n",
    "        target = self.data[idx+1:idx+self.context_length+1]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "context_length = 256\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TextDataset(train_data, context_length)\n",
    "val_dataset = TextDataset(val_data, context_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Check a sample batch\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Sample input: {tokenizer.decode(x[0].tolist()[:50])}...\")\n",
    "print(f\"Sample target: {tokenizer.decode(y[0].tolist()[:50])}...\")\n",
    "os.makedirs('results/part_4', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)"
   ],
   "id": "add0c1091caf9468",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mentions_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 48\u001B[39m\n\u001B[32m     45\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\n\u001B[32m     47\u001B[39m \u001B[38;5;66;03m# Create a tokenizer from the mentions text\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m tokenizer = CharacterTokenizer(\u001B[43mmentions_text\u001B[49m)\n\u001B[32m     50\u001B[39m \u001B[38;5;66;03m# Encode the entire text\u001B[39;00m\n\u001B[32m     51\u001B[39m encoded_text = tokenizer.encode(mentions_text)\n",
      "\u001B[31mNameError\u001B[39m: name 'mentions_text' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's implement a small GPT model (nanoGPT)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention module\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the multi-head attention module\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "        \n",
    "        # Key, query, value projections\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask to ensure that attention is only applied to the left\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.tril(torch.ones(context_length, context_length))\n",
    "            .view(1, 1, context_length, context_length)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len, n_embd = x.size()\n",
    "        \n",
    "        # Calculate query, key, values\n",
    "        q = self.query(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        # (batch_size, n_head, seq_len, head_dim) x (batch_size, n_head, head_dim, seq_len)\n",
    "        # -> (batch_size, n_head, seq_len, seq_len)\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        attn = attn.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # (batch_size, n_head, seq_len, seq_len) x (batch_size, n_head, seq_len, head_dim)\n",
    "        # -> (batch_size, n_head, seq_len, head_dim)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # Reshape and project\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embd)\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the feed-forward network\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the transformer block\n",
    "        \n",
    "        Args:\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MultiHeadAttention(n_embd, n_head, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ff = FeedForward(n_embd, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class NanoGPT(nn.Module):\n",
    "    \"\"\"Small GPT model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the nanoGPT model\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            n_embd: Embedding dimension\n",
    "            n_head: Number of attention heads\n",
    "            n_layer: Number of transformer blocks\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Position embedding\n",
    "        self.position_embedding = nn.Embedding(context_length, n_embd)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd, n_head, dropout) for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Output head\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Print model size\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"NanoGPT model with {n_params/1e6:.2f}M parameters\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len = idx.size()\n",
    "        \n",
    "        # Get token and position embeddings\n",
    "        token_emb = self.token_embedding(idx)\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Apply output head\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text from the model\n",
    "        \n",
    "        Args:\n",
    "            idx: Starting token IDs (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            temperature: Temperature for sampling (higher = more random)\n",
    "            \n",
    "        Returns:\n",
    "            Generated token IDs\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Crop context if it's too long\n",
    "                idx_cond = idx if idx.size(1) <= context_length else idx[:, -context_length:]\n",
    "                \n",
    "                # Get predictions\n",
    "                logits = self(idx_cond)\n",
    "                \n",
    "                # Focus on the last token\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                \n",
    "                # Apply softmax to get probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Sample from the distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Append to the sequence\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# Create a small nanoGPT model\n",
    "model = NanoGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_embd=128,\n",
    "    n_head=4,\n",
    "    n_layer=4,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "47e91de3eabfbc76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's train the nanoGPT model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=3e-4):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        epochs: Number of epochs to train for\n",
    "        lr: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Initialize loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_perplexity': [],\n",
    "        'val_perplexity': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for x, y in train_pbar:\n",
    "            # Move data to device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            logits = logits.view(-1, tokenizer.vocab_size)\n",
    "            y = y.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({'loss': train_loss / train_batches})\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_perplexity = np.exp(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        # Progress bar for validation\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_pbar:\n",
    "                # Move data to device\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = model(x)\n",
    "                \n",
    "                # Reshape for loss calculation\n",
    "                logits = logits.view(-1, tokenizer.vocab_size)\n",
    "                y = y.view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(logits, y)\n",
    "                \n",
    "                # Update statistics\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_pbar.set_postfix({'loss': val_loss / val_batches})\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_perplexity = np.exp(avg_val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Train Perplexity: {train_perplexity:.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Perplexity: {val_perplexity:.4f}\")\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_perplexity'].append(train_perplexity)\n",
    "        history['val_perplexity'].append(val_perplexity)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, f'models/nanogpt_checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'models/nanogpt.pt')\n",
    "    print(\"Model saved to models/nanogpt.pt\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Adjust based on your computational resources\n",
    "history = train_model(model, train_loader, val_loader, epochs=epochs)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot perplexity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_perplexity'], label='Train')\n",
    "plt.plot(history['val_perplexity'], label='Validation')\n",
    "plt.title('Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/part_4/training_history.png')\n",
    "plt.show()\n",
    "\n",
    "# Save training metrics\n",
    "with open('results/part_4/training_metrics.txt', 'w') as f:\n",
    "    f.write(\"# NanoGPT Training Metrics\\n\\n\")\n",
    "    f.write(\"## Model Configuration\\n\")\n",
    "    f.write(f\"Vocabulary Size: {tokenizer.vocab_size}\\n\")\n",
    "    f.write(f\"Embedding Dimension: {model.n_embd}\\n\")"
   ],
   "id": "1656bc943bd70a83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text from the model\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: The prompt text\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "        temperature: Temperature for sampling (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    # Encode the prompt\n",
    "    encoded_prompt = tokenizer.encode(prompt)\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    x = torch.tensor([encoded_prompt], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    output = model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output[0].tolist())\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('models/nanogpt.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Generate text with different prompts\n",
    "prompts = [\n",
    "    \"Diabetes is a chronic condition that\",\n",
    "    \"The symptoms of heart disease include\",\n",
    "    \"To prevent respiratory infections,\",\n",
    "    \"The treatment for hypertension typically involves\",\n",
    "    \"Mental health disorders are characterized by\"\n",
    "]\n",
    "\n",
    "# Generate and print text for each prompt\n",
    "print(\"Generated Text Samples:\")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "    generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=100)\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "\n",
    "# Compare with larger pre-trained models\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # Load a pre-trained model\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "    \n",
    "    print(\"\\n\\nComparison with GPT-2:\")\n",
    "    for i, prompt in enumerate(prompts[:2]):  # Just try a couple of prompts\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        \n",
    "        # Generate with our nanoGPT\n",
    "        nano_text = generate_text(model, tokenizer, prompt, max_new_tokens=50)\n",
    "        print(f\"NanoGPT: {nano_text}\")\n",
    "        \n",
    "        # Generate with GPT-2\n",
    "        gpt2_text = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
    "        print(f\"GPT-2: {gpt2_text}\")\n",
    "except:\n",
    "    print(\"\\nSkipping comparison with pre-trained models (requires internet connection)\")\n",
    "\n",
    "# Evaluate the quality of generated text\n",
    "def evaluate_generated_text(generated_samples):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of generated text\n",
    "    \n",
    "    Args:\n",
    "        generated_samples: List of generated text samples\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Simple metrics for text quality\n",
    "    metrics = {\n",
    "        'avg_length': 0,\n",
    "        'unique_words': 0,\n",
    "        'repetition_rate': 0\n",
    "    }\n",
    "    \n",
    "    total_length = 0\n",
    "    total_unique_words = 0\n",
    "    total_repetition_rate = 0\n",
    "    \n",
    "    for text in generated_samples:\n",
    "        # Calculate length\n",
    "        words = text.split()\n",
    "        length = len(words)\n",
    "        total_length += length\n",
    "        \n",
    "        # Calculate unique words\n",
    "        unique_words = len(set(words))\n",
    "        total_unique_words += unique_words\n",
    "        \n",
    "        # Calculate repetition rate (lower is better)\n",
    "        if length > 0:\n",
    "            repetition_rate = 1 - (unique_words / length)\n",
    "        else:\n",
    "            repetition_rate = 0\n",
    "        total_repetition_rate += repetition_rate\n",
    "    \n",
    "    # Calculate averages\n",
    "    n_samples = len(generated_samples)\n",
    "    if n_samples > 0:\n",
    "        metrics['avg_length'] = total_length / n_samples\n",
    "        metrics['unique_words'] = total_unique_words / n_samples\n",
    "        metrics['repetition_rate'] = total_repetition_rate / n_samples\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Generate a larger set of samples for evaluation\n",
    "evaluation_prompts = [\n",
    "    \"The patient presented with\",\n",
    "    \"Common side effects include\",\n",
    "    \"The diagnosis was confirmed by\",\n",
    "    \"Treatment options for this condition\",\n",
    "    \"The prognosis for patients with\"\n",
    "]\n",
    "\n",
    "generated_samples = []\n",
    "for prompt in evaluation_prompts:\n",
    "    for temp in [0.7, 0.8, 0.9]:  # Try different temperatures\n",
    "        generated_text = generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=temp)\n",
    "        generated_samples.append(generated_text)\n",
    "\n",
    "# Evaluate the generated samples\n",
    "evaluation_metrics = evaluate_generated_text(generated_samples)\n",
    "print(\"\\nGenerated Text Evaluation:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open('results/part_4/generation_evaluation.txt', 'w') as f:\n",
    "    f.write(\"# NanoGPT Text Generation Evaluation\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Evaluation Metrics\\n\")\n",
    "    for metric, value in evaluation_metrics.items():\n",
    "        f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\n## Generated Samples\\n\")\n",
    "    for i, (prompt, sample) in enumerate(zip(evaluation_prompts * 3, generated_samples)):\n",
    "        f.write(f\"\\nSample {i+1}:\\n\")\n",
    "        f.write(f\"Prompt: {prompt}\\n\")\n",
    "        f.write(f\"Generated: {sample}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"Evaluation results saved to results/part_4/generation_evaluation.txt\")"
   ],
   "id": "6d4edb18d77fa6cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
